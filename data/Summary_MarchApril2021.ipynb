{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summary_JanFeb2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#notebook to download the csv of edges and nodes of a given network\n",
        "import os\n",
        "import requests \n",
        "import time\n",
        "import string\n",
        "import networkx as nx\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.corpus import wordnet as wn #importing it\n",
        "from nltk.stem.wordnet import WordNetLemmatizer #importing wordnet lemmatizer\n",
        "from nltk import pos_tag #part-of-speech-tagger\n",
        "from collections import defaultdict #defaultdict returns default value for non-existant keys you try to  access based on the function you passed in the constructor\n",
        "from google.colab import files\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRcCylkZqDPi",
        "outputId": "2fe79134-9577-4d31-853c-db5657afa87e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(df):       #extract the text from the tweets and RT\n",
        "                            #works ONLY on .csv file\n",
        "  list_strings = []\n",
        "  for index in range(len(df)):\n",
        "    if index % 1000 == 0:\n",
        "      print(str(index)+' / '+str(len(df)))\n",
        "    text = df.loc[index]['text']                          #if it is nor trucated nor a RT  i take \"text\"\n",
        "    string = -1\n",
        "    if (df.loc[index,\"truncated\"] == True):                 #if it is trucated I take \"extended_tweet\"\n",
        "        string = df.loc[index,\"extended_tweet\"]\n",
        "    if type(df.loc[index,\"retweeted_status\"]) != float:     #if it is a RT I take retweeted_status\n",
        "        string = df.loc[index,\"retweeted_status\"]\n",
        "    if type(string) == str :\n",
        "        if(re.search('full_text\\':(.+?)https',string) != None):     #if I find \"full_text\"\n",
        "          s = re.search('full_text\\':(.+?)https',string).group(1)\n",
        "        if(re.search('text\\':(.+?)https',string)!= None):\n",
        "          s = re.search('text\\':(.+?)https',string).group(1)\n",
        "        else: \n",
        "          continue\n",
        "        list_strings.append(s)\n",
        "        #print(s)         \n",
        "    else:\n",
        "      list_strings.append(text)\n",
        "      #print(text)\n",
        "      \n",
        "\n",
        "  return list_strings"
      ],
      "metadata": {
        "id": "CGa_FVVGqDSJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning, lemmatising and pos tagging tweets\n",
        "\n",
        "nltk.download('words')\n",
        "WORDS = set(nltk.corpus.words.words()) #the last two lines serve to download the corpus of standard English language words\n",
        "nltk.download('stopwords') #downloading stopwords\n",
        "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\")) #taking the stop words from English language\n",
        "nltk.download('wordnet') #downloading wordnet\n",
        "nltk.download('averaged_perceptron_tagger') #downloading tagger\n",
        "tag_map = defaultdict(lambda : wn.NOUN) #here we define that wn.NOUN is the default value for the dict\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "def lemma_pos_cleaner(tweet):\n",
        "\n",
        "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    tweet = \" \".join(tweet.split())\n",
        "    tweet = str.lower(tweet) #to lowercase \n",
        "    tweet = re.sub(\"'\",\" \",tweet) # remove aphostrophe\n",
        "\n",
        "    #basically we use pos_tag function on tokens that we get by applying wordpunct tokenization\n",
        "    #to tweet (it separates all the words and symbols)\n",
        "    #then we pass the token along with it's wordnet pos value that we get from the tag_map dictionary (noun, adjective, verb or adverb) to the lemma function (the WordNetLemmatizer())\n",
        "    lemma_function = WordNetLemmatizer()\n",
        "    tweet = \" \".join(lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(tweet))) #lemmatize\n",
        "  \n",
        "\n",
        "    # francesco: I removed also all 2 letters words and added specific words, words that appears frequently but are discarded because they are not in the english language\n",
        "    SPECIFIC_WORDS = ['virus', 'coronavirus', 'covid19', 'covid', 'trump', 'hubei', 'beijing', 'xinjiang', 'jinping', 'korea', 'xinhua', 'india', 'taiwan','johnson','singapore', 'africa', 'japanese', 'france', 'asian', 'australia', 'french', 'asia', 'leishenshan', 'british', 'qingdao', 'fauci', 'america',  'california', 'sichuan', 'malaysia', 'huawei','thailand', 'shandong', 'italy', 'philippines', 'germany', 'facebook', 'african', 'shenzhen', 'tokyo', 'russian','uygur', '5g', 'pompeo', 'vietnam', 'australian', 'cambodia', 'zhejiang', 'yunnan', 'guangdong', 'korean', 'iran', 'washington']\n",
        "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) if (w in WORDS or w in SPECIFIC_WORDS) and len(w)>2 and w not in STOP_WORDS ) #remove stop words\n",
        "   \n",
        "    return tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0LDoYVqDUh",
        "outputId": "aa092e05-4ffc-4bb9-c476-c38e6bce8467"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_dictionary(df):\n",
        "  unique_words = {}\n",
        "\n",
        "  for row in df:\n",
        "    for word in row.split():\n",
        "      #if the word is encountered for the first time add to dict as key and set its value to 0\n",
        "      unique_words.setdefault(word,0)\n",
        "      #increase the value (i.e the count) of the word by 1 every time it is encountered\n",
        "      unique_words[word] += 1\n",
        "\n",
        "  return unique_words"
      ],
      "metadata": {
        "id": "MAfOr8vR4P88"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "def clean_words(cleaned_text, unuseful_words):\n",
        "  re_cleaned_text = cleaned_text.copy()\n",
        "  for txt in range(len(re_cleaned_text)):\n",
        "    if txt % 2000 == 0:\n",
        "      print(txt, '/',len(re_cleaned_text))\n",
        "    w = re_cleaned_text[txt].split()\n",
        "    for word in unuseful_words:\n",
        "      while word in w:\n",
        "        w.remove(word)\n",
        "    re_cleaned_text[txt] = ' '.join(w)\n",
        "  return re_cleaned_text"
      ],
      "metadata": {
        "id": "_qrTtTxDgYKz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period = '_MarchApril2021'  # '', '_JanFeb2020', '_MarchApril2021', '_SeptOct2020'"
      ],
      "metadata": {
        "id": "o-GUlzAVOwk_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "China = pd.read_csv('/content/China'+period+'.csv')\n",
        "USA = pd.read_csv('/content/USA'+period+'.csv')\n",
        "China_USA = pd.read_csv('/content/China&USA'+period+'.csv')"
      ],
      "metadata": {
        "id": "suPdLaUx1Ct8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of tweets:\n",
        "print('China: ', len(China))\n",
        "print('USA: ', len(USA))\n",
        "print('China&USA: ', len(China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeO9nxO6Br3x",
        "outputId": "2bdfb2d6-64c1-4c23-de45-37f05826014a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  1330\n",
            "USA:  3932\n",
            "China&USA:  5262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_China = extract_text(China)\n",
        "text_USA = extract_text(USA)\n",
        "text_China_USA = extract_text(China_USA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WI3Rz9T1Cxo",
        "outputId": "e37db156-950b-4177-e912-3515c2fec818"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 1330\n",
            "1000 / 1330\n",
            "0 / 3932\n",
            "1000 / 3932\n",
            "2000 / 3932\n",
            "3000 / 3932\n",
            "0 / 5262\n",
            "1000 / 5262\n",
            "2000 / 5262\n",
            "3000 / 5262\n",
            "4000 / 5262\n",
            "5000 / 5262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text_China = [lemma_pos_cleaner(txt) for txt in text_China]\n",
        "cleaned_text_USA = [lemma_pos_cleaner(txt) for txt in text_USA]\n",
        "cleaned_text_China_USA = [lemma_pos_cleaner(txt) for txt in text_China_USA]\n",
        "\n",
        "print('China:')\n",
        "print(cleaned_text_China[0:10])\n",
        "print()\n",
        "print('USA:')\n",
        "print(cleaned_text_USA[0:10])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(cleaned_text_China_USA[0:10])"
      ],
      "metadata": {
        "id": "6q4JI_jV1C2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02898fe-d4fa-473d-f7f3-1b7ccc56df2b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "['death beloved rapper protest killing black america anguish lose lost one covid even historic celebration america pick photo week', 'german chancellor receive first dose covid vaccine announce via government spokesman people daily', 'number suicide drop study find among black', 'total covid jab give far close vaccine administer hour period accord government data', 'would like see covid passport infringe people freedom health passport claim', 'lift ban flight continue easing covid restriction', 'covid pandemic result permanent closure around extra business compare', 'german chancellor receive first dose vaccine', 'report new confirm case', 'million receive least one dose covid vaccine drug develop']\n",
            "\n",
            "USA:\n",
            "['decoration fill street old city welcome holy month freer covid', 'migrant worker pile rail station india head back home village covid curb', 'health economics education world change since covid', 'decoration fill street old city welcome holy month freer covid restriction height pandemic', 'brazil covid variant mutate way make resistant vaccine scientist say', 'serum institute india world big vaccine maker urge lift export embargo', 'line japan slide back covid emergency', 'brazil covid variant mutate way make resistant vaccine scientist say know', 'third vaccine dose likely need within month say', 'india report another record daily rise covid infection']\n",
            "\n",
            "China&USA:\n",
            "['death beloved rapper protest killing black america anguish lose lost one covid even historic celebration america pick photo week', 'german chancellor receive first dose covid vaccine announce via government spokesman people daily', 'number suicide drop study find among black', 'total covid jab give far close vaccine administer hour period accord government data', 'would like see covid passport infringe people freedom health passport claim', 'lift ban flight continue easing covid restriction', 'covid pandemic result permanent closure around extra business compare', 'german chancellor receive first dose vaccine', 'report new confirm case', 'million receive least one dose covid vaccine drug develop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCiw0q7t4UER",
        "outputId": "78732a0e-1099-4b6e-9a83-66b2e1897213"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  1961\n",
            "USA:  3330\n",
            "China&USA:  3840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most frequent words\n",
        "print('China')\n",
        "print([key for key in freq_dict_China.keys() if freq_dict_China[key]>100])\n",
        "print()\n",
        "print('USA')\n",
        "print([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>100])\n",
        "print()\n",
        "print('China&USA')\n",
        "print([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y29OU9Cd5Auv",
        "outputId": "a2845d01-109c-4d7e-b40c-46bf62951f74"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "['covid', 'vaccine', 'china', 'say', 'case', 'country', 'receive', 'health', 'report', 'first', 'million', 'new', 'dos', 'people', 'pandemic', 'batch']\n",
            "\n",
            "USA\n",
            "['covid', 'vaccine', 'say', 'new', 'case', 'johnson', 'shot', 'million', 'use', 'report', 'people', 'get', 'india', 'country', 'first', 'health', 'dos', 'blood', 'vaccination', 'clot', 'death', 'coronavirus', 'pandemic', 'state', 'year', 'brazil', 'surge', 'president', 'world', 'infection', 'day', 'risk', 'record', 'trial', 'may', 'china', 'receive', 'official', 'rise', 'month', 'variant', 'vaccinate', 'high', 'study', 'plan', 'one', 'test', 'need', 'rare', 'week', 'make', 'could', 'amid']\n",
            "\n",
            "China&USA\n",
            "['covid', 'vaccine', 'say', 'new', 'case', 'china', 'million', 'report', 'country', 'first', 'health', 'people', 'johnson', 'dos', 'use', 'pandemic', 'shot', 'receive', 'get', 'vaccination', 'death', 'state', 'blood', 'coronavirus', 'india', 'clot', 'year', 'brazil', 'president', 'world', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# less frequent words: not performed on data with only covid, coronavirus, vaccine keys"
      ],
      "metadata": {
        "id": "9oHPXYbpINWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less frequent words:\n",
        "thr = 2\n",
        "print('Less frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]<thr]))\n",
        "print('More frequent China: ', len([key for key in freq_dict_China.keys() if freq_dict_China[key]>=thr]))\n",
        "less_frequent_words_China = [key for key in freq_dict_China.keys() if freq_dict_China[key]<thr]\n",
        "print()\n",
        "print('Less frequent USA: ', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]<thr]))\n",
        "print('More frequent USA:', len([key for key in freq_dict_USA.keys() if freq_dict_USA[key]>=thr]))\n",
        "less_frequent_words_USA = [key for key in freq_dict_USA.keys() if freq_dict_USA[key]<thr]\n",
        "print()\n",
        "print('Less frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<thr]))\n",
        "print('More frequent China&USA: ', len([key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]>=thr]))\n",
        "less_frequent_words_China_USA = [key for key in freq_dict_China_USA.keys() if freq_dict_China_USA[key]<thr]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgiQzaHD5zAb",
        "outputId": "fb9e2fde-5a52-4ce5-a9ad-402ceec94818"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Less frequent China:  1132\n",
            "More frequent China:  1761\n",
            "\n",
            "Less frequent USA:  820\n",
            "More frequent USA: 1920\n",
            "\n",
            "Less frequent China&USA:  1288\n",
            "More frequent China&USA:  2658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# discard less frequent words\n",
        "def clean_words(cleaned_text, unuseful_words):\n",
        "  re_cleaned_text = cleaned_text.copy()\n",
        "  for txt in range(len(re_cleaned_text)):\n",
        "    if txt % 2000 == 0:\n",
        "      print(txt, '/',len(re_cleaned_text))\n",
        "    w = re_cleaned_text[txt].split()\n",
        "    for word in unuseful_words:\n",
        "      while word in w:\n",
        "        w.remove(word)\n",
        "    re_cleaned_text[txt] = ' '.join(w)\n",
        "  return re_cleaned_text"
      ],
      "metadata": {
        "id": "wGHsUAKwkg4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_mostfreq_text_China = clean_words(cleaned_text_China, less_frequent_words_China)\n",
        "cleaned_mostfreq_text_USA = clean_words(cleaned_text_USA, less_frequent_words_USA)\n",
        "cleaned_mostfreq_text_China_USA = clean_words(cleaned_text_China_USA, less_frequent_words_China_USA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmlqtzHp6lh8",
        "outputId": "337c39ff-9fa1-4262-992c-f64708f0b7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 2438\n",
            "2000 / 2438\n",
            "0 / 3185\n",
            "2000 / 3185\n",
            "0 / 5623\n",
            "2000 / 5623\n",
            "4000 / 5623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostfreq_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostfreq_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostfreq_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KHj5-o57f8y",
        "outputId": "a93b1c2f-f043-4cda-8694-4ed382162367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  497\n",
            "USA:  576\n",
            "China&USA:  941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Network"
      ],
      "metadata": {
        "id": "1davLfA3HNV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxPPnevj890b",
        "outputId": "c9ad8252-fac4-4574-e591-65af1a820ebd"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         covid    942\n",
            "1       vaccine    757\n",
            "2         china    313\n",
            "3           say    175\n",
            "4          case    166\n",
            "5       country    162\n",
            "6       receive    159\n",
            "7        health    156\n",
            "8        report    156\n",
            "9         first    155\n",
            "10      million    149\n",
            "11          new    148\n",
            "12          dos    122\n",
            "13       people    117\n",
            "14     pandemic    109\n",
            "15        batch    105\n",
            "16    president     77\n",
            "17        world     77\n",
            "18         dose     71\n",
            "19          use     69\n",
            "20       accord     68\n",
            "21     national     67\n",
            "22          day     64\n",
            "23          one     63\n",
            "24       brazil     63\n",
            "25  vaccination     63\n",
            "26         year     62\n",
            "27        death     61\n",
            "28       second     59\n",
            "29   administer     58\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0         covid   3006\n",
            "1       vaccine   2114\n",
            "2           say    869\n",
            "3           new    412\n",
            "4          case    392\n",
            "5       johnson    298\n",
            "6          shot    258\n",
            "7       million    257\n",
            "8           use    249\n",
            "9        report    246\n",
            "10       people    246\n",
            "11          get    246\n",
            "12        india    236\n",
            "13      country    235\n",
            "14        first    235\n",
            "15       health    229\n",
            "16          dos    220\n",
            "17        blood    215\n",
            "18  vaccination    215\n",
            "19         clot    213\n",
            "20        death    209\n",
            "21  coronavirus    208\n",
            "22     pandemic    207\n",
            "23        state    207\n",
            "24         year    176\n",
            "25       brazil    167\n",
            "26        surge    156\n",
            "27    president    148\n",
            "28        world    141\n",
            "29    infection    141\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         covid   3948\n",
            "1       vaccine   2871\n",
            "2           say   1044\n",
            "3           new    560\n",
            "4          case    558\n",
            "5         china    440\n",
            "6       million    406\n",
            "7        report    402\n",
            "8       country    397\n",
            "9         first    390\n",
            "10       health    385\n",
            "11       people    363\n",
            "12      johnson    346\n",
            "13          dos    342\n",
            "14          use    318\n",
            "15     pandemic    316\n",
            "16         shot    289\n",
            "17      receive    286\n",
            "18          get    284\n",
            "19  vaccination    278\n",
            "20        death    270\n",
            "21        state    262\n",
            "22        blood    255\n",
            "23  coronavirus    248\n",
            "24        india    247\n",
            "25         clot    244\n",
            "26         year    238\n",
            "27       brazil    230\n",
            "28    president    225\n",
            "29        world    218\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "zHmEpbLtr46D"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network(cleaned_text):\n",
        "  network = {}\n",
        "  #connect the word that appear in the same tweets\n",
        "  for row in cleaned_text:\n",
        "    combined_list = [word for word in str.split(row)]\n",
        "    #for pair in itertools.product(combined_list, combined_list):\n",
        "    #print(combined_list)\n",
        "    for pair in itertools.product(combined_list, combined_list):\n",
        "          #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
        "          if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
        "              network.setdefault(pair,0)\n",
        "              network[pair] += 1 \n",
        "  network_df = pd.DataFrame.from_dict(network, orient=\"index\")\n",
        "  network_df.columns = [\"weight\"]\n",
        "  network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "  return network, network_df"
      ],
      "metadata": {
        "id": "nUAWB6lT1DAd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_text_China_USA)"
      ],
      "metadata": {
        "id": "7vR_r37vAGye"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxMWDZua_b-l",
        "outputId": "6549cfb2-cbdf-41a3-9bc8-3804b7ed542e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                    weight\n",
            "(covid, vaccine)       524\n",
            "(vaccine, china)       254\n",
            "(china, covid)         243\n",
            "(receive, vaccine)     171\n",
            "(covid, case)          140\n",
            "(million, covid)       140\n",
            "(country, vaccine)     134\n",
            "(dos, vaccine)         134\n",
            "(vaccine, say)         132\n",
            "(covid, say)           126\n",
            "(covid, health)        126\n",
            "(first, vaccine)       125\n",
            "(covid, country)       124\n",
            "(new, covid)           124\n",
            "(first, covid)         119\n",
            "(receive, covid)       118\n",
            "(covid, report)        114\n",
            "(million, vaccine)     112\n",
            "(batch, vaccine)       109\n",
            "(dos, covid)           104\n",
            "(vaccine, health)      103\n",
            "(covid, pandemic)      103\n",
            "(new, case)             97\n",
            "(covid, people)         92\n",
            "(batch, covid)          90\n",
            "(million, dos)          89\n",
            "(use, vaccine)          82\n",
            "(report, case)          77\n",
            "(dose, vaccine)         73\n",
            "(vaccine, people)       68\n",
            "\n",
            "USA:\n",
            "                      weight\n",
            "(covid, vaccine)        1450\n",
            "(covid, say)             641\n",
            "(vaccine, say)           626\n",
            "(new, covid)             362\n",
            "(covid, case)            352\n",
            "(johnson, vaccine)       287\n",
            "(johnson, covid)         239\n",
            "(use, vaccine)           232\n",
            "(clot, vaccine)          231\n",
            "(blood, vaccine)         230\n",
            "(vaccine, dos)           224\n",
            "(report, covid)          218\n",
            "(million, covid)         197\n",
            "(blood, clot)            195\n",
            "(covid, death)           193\n",
            "(covid, first)           192\n",
            "(country, covid)         192\n",
            "(covid, shot)            192\n",
            "(covid, get)             184\n",
            "(india, covid)           184\n",
            "(million, vaccine)       184\n",
            "(use, covid)             182\n",
            "(health, covid)          179\n",
            "(covid, dos)             174\n",
            "(people, covid)          164\n",
            "(shot, vaccine)          163\n",
            "(state, covid)           162\n",
            "(covid, vaccination)     161\n",
            "(get, vaccine)           160\n",
            "(country, vaccine)       148\n",
            "\n",
            "China&USA:\n",
            "                    weight\n",
            "(covid, vaccine)      1974\n",
            "(covid, say)           767\n",
            "(vaccine, say)         758\n",
            "(covid, case)          492\n",
            "(new, covid)           486\n",
            "(dos, vaccine)         358\n",
            "(china, covid)         353\n",
            "(johnson, vaccine)     343\n",
            "(million, covid)       337\n",
            "(covid, report)        332\n",
            "(vaccine, china)       326\n",
            "(covid, country)       316\n",
            "(use, vaccine)         314\n",
            "(first, covid)         311\n",
            "(covid, health)        305\n",
            "(million, vaccine)     296\n",
            "(country, vaccine)     282\n",
            "(receive, vaccine)     280\n",
            "(dos, covid)           278\n",
            "(vaccine, blood)       272\n",
            "(vaccine, clot)        264\n",
            "(johnson, covid)       257\n",
            "(covid, people)        256\n",
            "(death, covid)         251\n",
            "(first, vaccine)       251\n",
            "(covid, pandemic)      246\n",
            "(vaccine, health)      242\n",
            "(receive, covid)       233\n",
            "(use, covid)           233\n",
            "(blood, clot)          230\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph\n"
      ],
      "metadata": {
        "id": "gfvo8x0Ku78D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(network):\n",
        "  G = nx.Graph()\n",
        "  for edge in network:\n",
        "      G.add_edge(edge[0], edge[1], weight=network[edge])\n",
        "  return G"
      ],
      "metadata": {
        "id": "2NGKuSuYAo-K"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "TbpTF19bBKJq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "id": "W178ljb9rM6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36fddfb4-5913-4128-b2da-1654aec17a57"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  1960\n",
            "Edges:  40024\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  3330\n",
            "Edges:  98919\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  3839\n",
            "Edges:  126667\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PageRank"
      ],
      "metadata": {
        "id": "1LYNr4McLtWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "P6qJVv9vHIr1"
      },
      "outputs": [],
      "source": [
        "# Calculating the pagerank on graph G, teleportation probability here is 0.15 but since the graph is strongly connected we can set it to zero if we want\n",
        "pr_China = nx.algorithms.pagerank(G_China,alpha = 1)\n",
        "pr_China = dict(sorted(pr_China.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_USA = nx.algorithms.pagerank(G_USA,alpha = 1)\n",
        "pr_USA = dict(sorted(pr_USA.items(), key=lambda item: item[1],reverse  = True))\n",
        "\n",
        "pr_China_USA = nx.algorithms.pagerank(G_China_USA,alpha = 1)\n",
        "pr_China_USA = dict(sorted(pr_China_USA.items(), key=lambda item: item[1],reverse  = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "BcVoAXxqpqLd"
      },
      "outputs": [],
      "source": [
        "def threshold(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] >= threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_reverse(vector,threshold):\n",
        "\n",
        "  l = [(el,vector[el]) for el in vector if vector[el] < threshold ]\n",
        "\n",
        "  return pd.DataFrame(l)"
      ],
      "metadata": {
        "id": "7-UFIOEJL6bY"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cUhquN72cI6",
        "outputId": "07baccf1-9acb-40c6-d104-8d4eb0a4fba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  480\n",
            "\n",
            "              0         1\n",
            "0         covid  0.059567\n",
            "1       vaccine  0.045744\n",
            "2         china  0.020886\n",
            "3           say  0.014056\n",
            "4       country  0.012877\n",
            "5        health  0.012334\n",
            "6          case  0.010848\n",
            "7        report  0.010766\n",
            "8         first  0.010181\n",
            "9       million  0.009981\n",
            "10      receive  0.009459\n",
            "11          new  0.009430\n",
            "12       people  0.008845\n",
            "13          dos  0.008282\n",
            "14     pandemic  0.007880\n",
            "15        world  0.005986\n",
            "16       accord  0.005306\n",
            "17        batch  0.005286\n",
            "18          day  0.005094\n",
            "19    president  0.005074\n",
            "20          use  0.005050\n",
            "21     national  0.005036\n",
            "22         year  0.004926\n",
            "23       brazil  0.004622\n",
            "24  vaccination  0.004621\n",
            "25        death  0.004500\n",
            "26         city  0.004445\n",
            "27        state  0.004341\n",
            "28         dose  0.004290\n",
            "29          one  0.004040\n",
            "\n",
            "USA:  512\n",
            "\n",
            "              0         1\n",
            "0         covid  0.057114\n",
            "1       vaccine  0.041153\n",
            "2           say  0.020410\n",
            "3           new  0.008660\n",
            "4          case  0.008049\n",
            "5       johnson  0.007497\n",
            "6        people  0.006510\n",
            "7        health  0.006364\n",
            "8       country  0.006360\n",
            "9           get  0.006201\n",
            "10         shot  0.006116\n",
            "11     pandemic  0.006054\n",
            "12  coronavirus  0.005859\n",
            "13        blood  0.005494\n",
            "14          use  0.005463\n",
            "15       report  0.005453\n",
            "16        state  0.005364\n",
            "17        first  0.005347\n",
            "18         clot  0.005201\n",
            "19  vaccination  0.005055\n",
            "20         year  0.005042\n",
            "21      million  0.004855\n",
            "22        death  0.004603\n",
            "23        india  0.004324\n",
            "24          dos  0.004050\n",
            "25        world  0.003729\n",
            "26          day  0.003708\n",
            "27        surge  0.003662\n",
            "28    president  0.003606\n",
            "29     official  0.003483\n",
            "\n",
            "China&USA:  501\n",
            "\n",
            "              0         1\n",
            "0         covid  0.057710\n",
            "1       vaccine  0.042272\n",
            "2           say  0.018876\n",
            "3           new  0.008848\n",
            "4          case  0.008726\n",
            "5       country  0.007937\n",
            "6        health  0.007808\n",
            "7        people  0.007076\n",
            "8        report  0.006739\n",
            "9         china  0.006651\n",
            "10        first  0.006517\n",
            "11     pandemic  0.006495\n",
            "12      johnson  0.006434\n",
            "13      million  0.006095\n",
            "14          use  0.005365\n",
            "15          get  0.005255\n",
            "16         shot  0.005157\n",
            "17        state  0.005117\n",
            "18          dos  0.005075\n",
            "19  coronavirus  0.005062\n",
            "20         year  0.005012\n",
            "21  vaccination  0.004952\n",
            "22        blood  0.004864\n",
            "23      receive  0.004660\n",
            "24        death  0.004578\n",
            "25         clot  0.004490\n",
            "26        world  0.004274\n",
            "27          day  0.004044\n",
            "28    president  0.003961\n",
            "29       brazil  0.003706\n"
          ]
        }
      ],
      "source": [
        "thr = 0.0004\n",
        "print('China: ', len(threshold(pr_China,thr)))\n",
        "print()\n",
        "print(threshold(pr_China,thr).iloc[:30])\n",
        "print()\n",
        "print('USA: ', len(threshold(pr_USA,thr)))\n",
        "print()\n",
        "print(threshold(pr_USA,thr).iloc[:30])\n",
        "print()\n",
        "print('China&USA: ', len(threshold(pr_China_USA,thr)))\n",
        "print()\n",
        "print(threshold(pr_China_USA,thr).iloc[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqlZ2nZ9qKW"
      },
      "source": [
        "# TF-IDF: not performed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVbC8VFA9pgw"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1))   # ngram range can be changed to obtain measures regarding n grams instead of single words\n",
        "\n",
        "X_China = tfidf.fit_transform(cleaned_text_China).toarray()    # entry (i,j) if Tfidf measure of word_list[j] in document i\n",
        "word_list_China = tfidf.get_feature_names_out()\n",
        "\n",
        "X_USA = tfidf.fit_transform(cleaned_text_USA).toarray()\n",
        "word_list_USA = tfidf.get_feature_names_out()\n",
        "\n",
        "X_China_USA = tfidf.fit_transform(cleaned_text_China_USA).toarray()\n",
        "word_list_China_USA = tfidf.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k8fcvKX-yah"
      },
      "outputs": [],
      "source": [
        "tfidf_df_China = pd.DataFrame(X_China,columns = word_list_China)\n",
        "\n",
        "tfidf_df_USA = pd.DataFrame(X_USA,columns = word_list_USA)\n",
        "\n",
        "tfidf_df_China_USA = pd.DataFrame(X_China_USA,columns = word_list_China_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUr9_93WCKsj"
      },
      "outputs": [],
      "source": [
        "tfidf_word_measure_China = np.mean(tfidf_df_China,axis = 0)\n",
        "tfidf_word_measure_China = tfidf_word_measure_China.sort_values(ascending = False)\n",
        "tfidf_word_measure_USA = np.mean(tfidf_df_USA,axis = 0)\n",
        "tfidf_word_measure_USA = tfidf_word_measure_USA.sort_values(ascending = False)\n",
        "tfidf_word_measure_China_USA = np.mean(tfidf_df_China_USA,axis = 0)\n",
        "tfidf_word_measure_China_USA = tfidf_word_measure_China_USA.sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(tfidf_word_measure_China[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(tfidf_word_measure_USA[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(tfidf_word_measure_China_USA[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dknv21f3XimF",
        "outputId": "be7d3fd2-d1cb-41ef-af97-37a9994ef55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "covid          0.076069\n",
            "watch          0.059429\n",
            "coronavirus    0.047427\n",
            "case           0.044577\n",
            "china          0.037314\n",
            "say            0.037011\n",
            "new            0.035957\n",
            "test           0.032966\n",
            "report         0.031964\n",
            "president      0.029373\n",
            "pandemic       0.029153\n",
            "vaccine        0.026741\n",
            "trump          0.026549\n",
            "update         0.026117\n",
            "health         0.025600\n",
            "country        0.024406\n",
            "positive       0.023640\n",
            "million        0.020711\n",
            "world          0.020349\n",
            "people         0.020237\n",
            "death          0.019359\n",
            "accord         0.017701\n",
            "late           0.017458\n",
            "sept           0.016890\n",
            "year           0.016416\n",
            "global         0.016072\n",
            "day            0.015739\n",
            "number         0.015450\n",
            "national       0.014457\n",
            "infection      0.014375\n",
            "dtype: float64\n",
            "\n",
            "USA:\n",
            "covid          0.063696\n",
            "coronavirus    0.043593\n",
            "case           0.032740\n",
            "say            0.031375\n",
            "new            0.030973\n",
            "vaccine        0.029819\n",
            "trump          0.026958\n",
            "test           0.026622\n",
            "president      0.021916\n",
            "report         0.021543\n",
            "positive       0.017711\n",
            "pandemic       0.016017\n",
            "death          0.015852\n",
            "day            0.013833\n",
            "house          0.012634\n",
            "infection      0.012417\n",
            "million        0.012382\n",
            "health         0.012242\n",
            "rise           0.012128\n",
            "state          0.011852\n",
            "trial          0.011631\n",
            "people         0.011513\n",
            "record         0.011287\n",
            "china          0.011244\n",
            "first          0.010940\n",
            "world          0.010269\n",
            "white          0.010210\n",
            "week           0.009885\n",
            "month          0.009750\n",
            "one            0.009536\n",
            "dtype: float64\n",
            "\n",
            "China&USA:\n",
            "covid          0.060835\n",
            "coronavirus    0.041928\n",
            "case           0.033042\n",
            "new            0.030410\n",
            "say            0.029997\n",
            "vaccine        0.027990\n",
            "test           0.025911\n",
            "trump          0.025467\n",
            "report         0.022188\n",
            "president      0.021459\n",
            "positive       0.017466\n",
            "pandemic       0.016267\n",
            "death          0.015868\n",
            "day            0.013524\n",
            "china          0.013455\n",
            "health         0.013341\n",
            "million        0.012933\n",
            "house          0.012178\n",
            "infection      0.012158\n",
            "people         0.011836\n",
            "state          0.011514\n",
            "rise           0.011490\n",
            "watch          0.011118\n",
            "record         0.010997\n",
            "world          0.010822\n",
            "country        0.010764\n",
            "trial          0.010702\n",
            "first          0.010654\n",
            "white          0.009986\n",
            "week           0.009475\n",
            "dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reduced graph"
      ],
      "metadata": {
        "id": "4nkLA8k0LB7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# less important words:\n",
        "less_important_words_China = list(threshold_reverse(pr_China,thr)[0])\n",
        "\n",
        "less_important_words_USA = list(threshold_reverse(pr_USA,thr)[0])\n",
        "\n",
        "less_important_words_China_USA = list(threshold_reverse(pr_China_USA,thr)[0])"
      ],
      "metadata": {
        "id": "6ndwCL9oLpDA"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_mostimp_text_China = clean_words(cleaned_text_China,less_important_words_China)\n",
        "cleaned_mostimp_text_USA = clean_words(cleaned_text_USA,less_important_words_USA)\n",
        "cleaned_mostimp_text_China_USA = clean_words(cleaned_text_China_USA,less_important_words_China_USA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOA3UEyBunzo",
        "outputId": "b68370a4-0cdc-4a40-a5d4-af3718fffe1e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 1294\n",
            "0 / 3932\n",
            "2000 / 3932\n",
            "0 / 5226\n",
            "2000 / 5226\n",
            "4000 / 5226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict_China = frequency_dictionary(cleaned_mostimp_text_China)\n",
        "freq_dict_China = dict(sorted(freq_dict_China.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_USA = frequency_dictionary(cleaned_mostimp_text_USA)\n",
        "freq_dict_USA = dict(sorted(freq_dict_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "freq_dict_China_USA = frequency_dictionary(cleaned_mostimp_text_China_USA)\n",
        "freq_dict_China_USA = dict(sorted(freq_dict_China_USA.items(), key=lambda item: item[1], reverse = True))   #order from more frequent to less frequent word\n",
        "\n",
        "# number of words in the cleaned tweets:\n",
        "print('China: ', len(list(freq_dict_China)))\n",
        "print('USA: ', len(list(freq_dict_USA)))\n",
        "print('China&USA: ', len(list(freq_dict_China_USA)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7205e9d3-7fd9-4ba5-9914-a5b283dfa22c",
        "id": "X2ObCNtJLpDB"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:  481\n",
            "USA:  512\n",
            "China&USA:  502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_China = pd.DataFrame.from_dict(freq_dict_China, orient='index').reset_index()\n",
        "df_China.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China.reset_index(inplace=True)\n",
        "df_China.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_USA = pd.DataFrame.from_dict(freq_dict_USA, orient='index').reset_index()\n",
        "df_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_USA.reset_index(inplace=True)\n",
        "df_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "df_China_USA = pd.DataFrame.from_dict(freq_dict_China_USA, orient='index').reset_index()\n",
        "df_China_USA.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "df_China_USA.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "df_China_USA.reset_index(inplace=True)\n",
        "df_China_USA.drop(columns=\"index\",inplace=True)\n",
        "\n",
        "print('China')\n",
        "print(df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA')\n",
        "print(df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA')\n",
        "print(df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97fe9364-e0c0-40cf-890d-eaf54c775b49",
        "id": "DfrViAqzNnb9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China\n",
            "           Word  Count\n",
            "0         covid    942\n",
            "1       vaccine    757\n",
            "2         china    313\n",
            "3           say    175\n",
            "4          case    166\n",
            "5       country    162\n",
            "6       receive    159\n",
            "7        health    156\n",
            "8        report    156\n",
            "9         first    155\n",
            "10      million    149\n",
            "11          new    148\n",
            "12          dos    122\n",
            "13       people    117\n",
            "14     pandemic    109\n",
            "15        batch    105\n",
            "16    president     77\n",
            "17        world     77\n",
            "18         dose     71\n",
            "19          use     69\n",
            "20       accord     68\n",
            "21     national     67\n",
            "22          day     64\n",
            "23          one     63\n",
            "24       brazil     63\n",
            "25  vaccination     63\n",
            "26         year     62\n",
            "27        death     61\n",
            "28       second     59\n",
            "29   administer     58\n",
            "\n",
            "USA\n",
            "           Word  Count\n",
            "0         covid   3006\n",
            "1       vaccine   2114\n",
            "2           say    869\n",
            "3           new    412\n",
            "4          case    392\n",
            "5       johnson    298\n",
            "6          shot    258\n",
            "7       million    257\n",
            "8           use    249\n",
            "9        report    246\n",
            "10       people    246\n",
            "11          get    246\n",
            "12        india    236\n",
            "13      country    235\n",
            "14        first    235\n",
            "15       health    229\n",
            "16          dos    220\n",
            "17  vaccination    215\n",
            "18        blood    215\n",
            "19         clot    213\n",
            "20        death    209\n",
            "21  coronavirus    208\n",
            "22     pandemic    207\n",
            "23        state    207\n",
            "24         year    176\n",
            "25       brazil    167\n",
            "26        surge    156\n",
            "27    president    148\n",
            "28        world    141\n",
            "29    infection    141\n",
            "\n",
            "China&USA\n",
            "           Word  Count\n",
            "0         covid   3948\n",
            "1       vaccine   2871\n",
            "2           say   1044\n",
            "3           new    560\n",
            "4          case    558\n",
            "5         china    440\n",
            "6       million    406\n",
            "7        report    402\n",
            "8       country    397\n",
            "9         first    390\n",
            "10       health    385\n",
            "11       people    363\n",
            "12      johnson    346\n",
            "13          dos    342\n",
            "14          use    318\n",
            "15     pandemic    316\n",
            "16         shot    289\n",
            "17      receive    286\n",
            "18          get    284\n",
            "19  vaccination    278\n",
            "20        death    270\n",
            "21        state    262\n",
            "22        blood    255\n",
            "23  coronavirus    248\n",
            "24        india    247\n",
            "25         clot    244\n",
            "26         year    238\n",
            "27       brazil    230\n",
            "28    president    225\n",
            "29        world    218\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_China = freq_dict_China.keys()  \n",
        "keys_USA = freq_dict_USA.keys()  \n",
        "keys_China_USA = freq_dict_China_USA.keys()  "
      ],
      "metadata": {
        "id": "Q52SCkkKNncF"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_China, network_df_China = create_network(cleaned_mostimp_text_China)\n",
        "network_USA, network_df_USA = create_network(cleaned_mostimp_text_USA)\n",
        "network_China_USA, network_df_China_USA = create_network(cleaned_mostimp_text_China_USA)"
      ],
      "metadata": {
        "id": "bPn3tfv7NncF"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print(network_df_China.iloc[0:30])\n",
        "print()\n",
        "print('USA:')\n",
        "print(network_df_USA.iloc[0:30])\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(network_df_China_USA.iloc[0:30])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626dd8c2-ab90-49b1-e81f-7b0b4dda86d0",
        "id": "MVggBElVNncF"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "                    weight\n",
            "(covid, vaccine)       524\n",
            "(vaccine, china)       254\n",
            "(china, covid)         243\n",
            "(receive, vaccine)     171\n",
            "(covid, case)          140\n",
            "(million, covid)       140\n",
            "(country, vaccine)     134\n",
            "(dos, vaccine)         134\n",
            "(vaccine, say)         132\n",
            "(covid, health)        126\n",
            "(covid, say)           126\n",
            "(first, vaccine)       125\n",
            "(new, covid)           124\n",
            "(covid, country)       124\n",
            "(first, covid)         119\n",
            "(receive, covid)       118\n",
            "(covid, report)        114\n",
            "(million, vaccine)     112\n",
            "(batch, vaccine)       109\n",
            "(dos, covid)           104\n",
            "(vaccine, health)      103\n",
            "(covid, pandemic)      103\n",
            "(new, case)             97\n",
            "(covid, people)         92\n",
            "(batch, covid)          90\n",
            "(million, dos)          89\n",
            "(use, vaccine)          82\n",
            "(report, case)          77\n",
            "(dose, vaccine)         73\n",
            "(vaccine, people)       68\n",
            "\n",
            "USA:\n",
            "                      weight\n",
            "(covid, vaccine)        1450\n",
            "(covid, say)             641\n",
            "(vaccine, say)           626\n",
            "(new, covid)             362\n",
            "(covid, case)            352\n",
            "(johnson, vaccine)       287\n",
            "(johnson, covid)         239\n",
            "(use, vaccine)           232\n",
            "(clot, vaccine)          231\n",
            "(blood, vaccine)         230\n",
            "(vaccine, dos)           224\n",
            "(report, covid)          218\n",
            "(million, covid)         197\n",
            "(blood, clot)            195\n",
            "(covid, death)           193\n",
            "(country, covid)         192\n",
            "(covid, first)           192\n",
            "(covid, shot)            192\n",
            "(million, vaccine)       184\n",
            "(india, covid)           184\n",
            "(covid, get)             184\n",
            "(use, covid)             182\n",
            "(health, covid)          179\n",
            "(covid, dos)             174\n",
            "(people, covid)          164\n",
            "(shot, vaccine)          163\n",
            "(state, covid)           162\n",
            "(covid, vaccination)     161\n",
            "(get, vaccine)           160\n",
            "(country, vaccine)       148\n",
            "\n",
            "China&USA:\n",
            "                    weight\n",
            "(covid, vaccine)      1974\n",
            "(covid, say)           767\n",
            "(vaccine, say)         758\n",
            "(covid, case)          492\n",
            "(new, covid)           486\n",
            "(dos, vaccine)         358\n",
            "(china, covid)         353\n",
            "(johnson, vaccine)     343\n",
            "(million, covid)       337\n",
            "(covid, report)        332\n",
            "(vaccine, china)       326\n",
            "(covid, country)       316\n",
            "(use, vaccine)         314\n",
            "(first, covid)         311\n",
            "(covid, health)        305\n",
            "(million, vaccine)     296\n",
            "(country, vaccine)     282\n",
            "(receive, vaccine)     280\n",
            "(dos, covid)           278\n",
            "(vaccine, blood)       272\n",
            "(vaccine, clot)        264\n",
            "(johnson, covid)       257\n",
            "(covid, people)        256\n",
            "(first, vaccine)       251\n",
            "(death, covid)         251\n",
            "(covid, pandemic)      246\n",
            "(vaccine, health)      242\n",
            "(use, covid)           233\n",
            "(receive, covid)       233\n",
            "(blood, clot)          230\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G_China = get_graph(network_China)\n",
        "G_USA = get_graph(network_USA)\n",
        "G_China_USA = get_graph(network_China_USA)"
      ],
      "metadata": {
        "id": "zNrH5boVNncF"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('China:')\n",
        "print('Nodes: ',len(G_China.nodes()))\n",
        "print('Edges: ',len(G_China.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China))\n",
        "print()\n",
        "print('USA:')\n",
        "print('Nodes: ',len(G_USA.nodes()))\n",
        "print('Edges: ',len(G_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_USA))\n",
        "print()\n",
        "print('China&USA:')\n",
        "print('Nodes: ',len(G_China_USA.nodes()))\n",
        "print('Edges: ',len(G_China_USA.edges()))\n",
        "print('Is connected: ',nx.is_connected(G_China_USA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4f8e48-4ed3-4abd-9b51-fd5c68699b94",
        "id": "kx5041hFNncF"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "Nodes:  480\n",
            "Edges:  17152\n",
            "Is connected:  True\n",
            "\n",
            "USA:\n",
            "Nodes:  512\n",
            "Edges:  34347\n",
            "Is connected:  True\n",
            "\n",
            "China&USA:\n",
            "Nodes:  501\n",
            "Edges:  39757\n",
            "Is connected:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save edge list"
      ],
      "metadata": {
        "id": "p4SvWlzSfzFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './edgelist_China'+period+'.csv'\n",
        "nx.write_weighted_edgelist(G_China, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China__MarchApril2021.csv\n",
        "files.download('edgelist_China'+period+'.csv')\n",
        "\n",
        "filename = './edgelist_USA'+period+'.csv'\n",
        "nx.write_weighted_edgelist(G_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_USA__MarchApril2021.csv\n",
        "files.download('edgelist_USA'+period+'.csv')\n",
        "\n",
        "filename = './edgelist_China_USA'+period+'.csv'\n",
        "nx.write_weighted_edgelist(G_China_USA, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on colab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist_China_USA__MarchApril2021.csv\n",
        "files.download('edgelist_China_USA'+period+'.csv')"
      ],
      "metadata": {
        "id": "zgmmRGRCrM9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "398b6663-bc04-493e-c88a-4e0a6663bed4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: can't read ./edgelist_China__MarchApril2021.csv: No such file or directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_46fe1376-4685-4772-81a1-88a9e888e62d\", \"edgelist_China_MarchApril2021.csv\", 270109)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: can't read ./edgelist_USA__MarchApril2021.csv: No such file or directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f7b77cb6-7b88-46d8-bba6-d2c6f5c63990\", \"edgelist_USA_MarchApril2021.csv\", 527938)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: can't read ./edgelist_China_USA__MarchApril2021.csv: No such file or directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_61bb94d7-9280-4f20-8cf8-732228f758d7\", \"edgelist_China_USA_MarchApril2021.csv\", 614091)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Node List\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nWHLO7AhdwzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nodes(freq_dict, name):\n",
        "  word_nodes = pd.DataFrame.from_dict(freq_dict,orient=\"index\")\n",
        "  word_nodes.reset_index(inplace=True)\n",
        "  word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
        "  word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
        "  word_nodes = word_nodes.drop(columns=['delete'])\n",
        "  nodelist = pd.DataFrame()\n",
        "  nodelist = nodelist.append(word_nodes, ignore_index=True)\n",
        "\n",
        "  nodelist = nodelist.to_csv(\"nodelist_\"+name+\".csv\",index=False)\n",
        "  files.download(\"nodelist_\"+name+\".csv\")\n",
        "  return nodelist, word_nodes"
      ],
      "metadata": {
        "id": "v2GYb2BQFzET"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist_China, word_nodes_China = nodes(freq_dict_China,'China'+period)\n",
        "nodelist_USA, word_nodes_USA = nodes(freq_dict_USA,'USA_'+period)\n",
        "nodelist_China_USA, word_nodes_China_USA = nodes(freq_dict_China_USA,'China_USA'+period)\n",
        "\n",
        "print('China:')\n",
        "print(word_nodes_China.head())\n",
        "print()\n",
        "print('USA:')\n",
        "print(word_nodes_USA.head())\n",
        "print()\n",
        "print('China&USA:')\n",
        "print(word_nodes_China_USA.head())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "EmQrEHV0F1QY",
        "outputId": "03087c36-3f61-423b-feb7-d9907ae54033"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_127205c5-9114-4e12-aef4-cbbbd7bbd91e\", \"nodelist_China_MarchApril2021.csv\", 6749)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_39b5bb8c-6143-43f3-ad95-b0255c5dc99c\", \"nodelist_USA__MarchApril2021.csv\", 6973)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_095bc2f9-04cb-44a1-b627-b685141915b8\", \"nodelist_China_USA_MarchApril2021.csv\", 6865)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "China:\n",
            "        Id    Label\n",
            "0    covid    covid\n",
            "1  vaccine  vaccine\n",
            "2    china    china\n",
            "3      say      say\n",
            "4     case     case\n",
            "\n",
            "USA:\n",
            "        Id    Label\n",
            "0    covid    covid\n",
            "1  vaccine  vaccine\n",
            "2      say      say\n",
            "3      new      new\n",
            "4     case     case\n",
            "\n",
            "China&USA:\n",
            "        Id    Label\n",
            "0    covid    covid\n",
            "1  vaccine  vaccine\n",
            "2      say      say\n",
            "3      new      new\n",
            "4     case     case\n",
            "\n"
          ]
        }
      ]
    }
  ]
}